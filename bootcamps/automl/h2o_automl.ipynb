{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* source: https://github.com/h2oai/h2o-tutorials/tree/master/h2o-world-2017/automl\n",
    "\n",
    "Here’s an example showing basic usage of the h2o.automl() function in R and the H2OAutoML class in Python. For demonstration purposes only, we explicitly specify the the x argument, even though on this dataset, that’s not required. With this dataset, the set of predictors is all columns other than the response. Like other H2O algorithms, the default value of x is “all columns, excluding y”, so that will produce the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n"
     ]
    },
    {
     "ename": "H2OStartupError",
     "evalue": "Cannot find Java. Please install the latest JRE from\nhttp://www.oracle.com/technetwork/java/javase/downloads/index.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mH2OConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h2o/h2o.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(url, ip, port, name, https, cacert, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, log_dir, log_level, max_log_file_size, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, jvm_custom_args, bind_to_localhost, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m                                      \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                                      _msgs=(\"Checking whether there is an H2O instance running at {url} \",\n\u001b[0m\u001b[1;32m    291\u001b[0m                                             \"connected.\", \"not found.\"))\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h2o/backend/connection.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(server, url, ip, port, name, https, auth, verify_ssl_certificates, cacert, proxy, cookies, verbose, _msgs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0;31m# If a server is unable to respond within 1s, it should be considered a bug. However we disable this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h2o/backend/connection.py\u001b[0m in \u001b[0;36m_test_connection\u001b[0;34m(self, max_retries, messages)\u001b[0m\n\u001b[1;32m    683\u001b[0m             raise H2OConnectionError(\"Could not establish link to the H2O cloud %s after %d retries\\n%s\"\n\u001b[0;32m--> 684\u001b[0;31m                                      % (self._base_url, max_retries, \"\\n\".join(errors)))\n\u001b[0m\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mH2OConnectionError\u001b[0m: Could not establish link to the H2O cloud http://localhost:54321 after 5 retries\n[08:25.40] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f009815a210>: Failed to establish a new connection: [Errno 111] Connection refused'))\n[08:25.61] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f0088858c50>: Failed to establish a new connection: [Errno 111] Connection refused'))\n[08:25.81] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f0088861a90>: Failed to establish a new connection: [Errno 111] Connection refused'))\n[08:26.02] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f00888678d0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n[08:26.23] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Cloud (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f0088867910>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mH2OStartupError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4e201eb7e211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mH2OAutoML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h2o/h2o.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(url, ip, port, name, https, cacert, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, log_dir, log_level, max_log_file_size, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, jvm_custom_args, bind_to_localhost, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                                   \u001b[0mmax_log_file_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_log_file_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                                   \u001b[0mextra_classpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra_classpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjvm_custom_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjvm_custom_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                                   bind_to_localhost=bind_to_localhost)\n\u001b[0m\u001b[1;32m    308\u001b[0m         h2oconn = H2OConnection.open(server=hs, https=https, verify_ssl_certificates=verify_ssl_certificates,\n\u001b[1;32m    309\u001b[0m                                      cacert=cacert, auth=auth, proxy=proxy,cookies=cookies, verbose=True)\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h2o/backend/server.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(jar_path, nthreads, enable_assertions, max_mem_size, min_mem_size, ice_root, log_dir, log_level, max_log_file_size, port, name, extra_classpath, verbose, jvm_custom_args, bind_to_localhost)\u001b[0m\n\u001b[1;32m    141\u001b[0m         hs._launch_server(port=port, baseport=baseport, nthreads=int(nthreads), ea=enable_assertions,\n\u001b[1;32m    142\u001b[0m                           \u001b[0mmmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_mem_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_mem_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjvm_custom_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjvm_custom_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                           bind_to_localhost=bind_to_localhost, log_dir=log_dir, log_level=log_level, max_log_file_size=max_log_file_size)\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  Server is running at %s://%s:%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0matexit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h2o/backend/server.py\u001b[0m in \u001b[0;36m_launch_server\u001b[0;34m(self, port, baseport, mmax, mmin, ea, nthreads, jvm_custom_args, bind_to_localhost, log_dir, log_level, max_log_file_size)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# Find Java and check version. (Note that subprocess.check_output returns the output as a bytes object)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mjava\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h2o/backend/server.py\u001b[0m in \u001b[0;36m_find_java\u001b[0;34m()\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# not found...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         raise H2OStartupError(\"Cannot find Java. Please install the latest JRE from\\n\"\n\u001b[0m\u001b[1;32m    442\u001b[0m                               \"http://www.oracle.com/technetwork/java/javase/downloads/index.html\")\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mH2OStartupError\u001b[0m: Cannot find Java. Please install the latest JRE from\nhttp://www.oracle.com/technetwork/java/javase/downloads/index.html"
     ]
    }
   ],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import a sample binary outcome train/test set into H2O\n",
    "train = h2o.import_file(\"https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv\")\n",
    "test = h2o.import_file(\"https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify predictors and response\n",
    "x = train.columns\n",
    "y = \"response\"\n",
    "x.remove(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For binary classification, response should be a factor\n",
    "train[y] = train[y].asfactor()\n",
    "test[y] = test[y].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run AutoML for 20 base models (limited to 1 hour max runtime by default)\n",
    "aml = H2OAutoML(max_models=20, seed=1)\n",
    "aml.train(x=x, y=y, training_frame=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# View the AutoML Leaderboard\n",
    "lb = aml.leaderboard\n",
    "lb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_id                                                  auc    logloss    mean_per_class_error      rmse       mse\n",
    "# ---------------------------------------------------  --------  ---------  ----------------------  --------  --------\n",
    "# StackedEnsemble_AllModels_AutoML_20181212_105540     0.789801   0.551109                0.333174  0.43211   0.186719\n",
    "# StackedEnsemble_BestOfFamily_AutoML_20181212_105540  0.788425   0.552145                0.323192  0.432625  0.187165\n",
    "# XGBoost_1_AutoML_20181212_105540                     0.784651   0.55753                 0.325471  0.434949  0.189181\n",
    "# XGBoost_grid_1_AutoML_20181212_105540_model_4        0.783523   0.557854                0.318819  0.435249  0.189441\n",
    "# XGBoost_grid_1_AutoML_20181212_105540_model_3        0.783004   0.559613                0.325081  0.435708  0.189841\n",
    "# XGBoost_2_AutoML_20181212_105540                     0.78136    0.55888                 0.347074  0.435907  0.190015\n",
    "# XGBoost_3_AutoML_20181212_105540                     0.780847   0.559589                0.330739  0.43613   0.190209\n",
    "# GBM_5_AutoML_20181212_105540                         0.780837   0.559903                0.340848  0.436191  0.190263\n",
    "# GBM_2_AutoML_20181212_105540                         0.780036   0.559806                0.339926  0.436415  0.190458\n",
    "# GBM_1_AutoML_20181212_105540                         0.779827   0.560857                0.335096  0.436616  0.190633\n",
    "# GBM_3_AutoML_20181212_105540                         0.778669   0.56179                 0.325538  0.437189  0.191134\n",
    "# XGBoost_grid_1_AutoML_20181212_105540_model_2        0.774411   0.575017                0.322811  0.4427    0.195984\n",
    "# GBM_4_AutoML_20181212_105540                         0.771426   0.569712                0.33742   0.44107   0.194543\n",
    "# GBM_grid_1_AutoML_20181212_105540_model_1            0.769752   0.572583                0.344331  0.442452  0.195764\n",
    "# GBM_grid_1_AutoML_20181212_105540_model_2            0.754366   0.918567                0.355855  0.496638  0.246649\n",
    "# DRF_1_AutoML_20181212_105540                         0.742892   0.595883                0.355403  0.452774  0.205004\n",
    "# XRT_1_AutoML_20181212_105540                         0.742091   0.599346                0.356583  0.453117  0.205315\n",
    "# DeepLearning_grid_1_AutoML_20181212_105540_model_2   0.741795   0.601497                0.368291  0.454904  0.206937\n",
    "# XGBoost_grid_1_AutoML_20181212_105540_model_1        0.693554   0.620702                0.40588   0.465791  0.216961\n",
    "# DeepLearning_1_AutoML_20181212_105540                0.69137    0.637954                0.409351  0.47178   0.222576\n",
    "# DeepLearning_grid_1_AutoML_20181212_105540_model_1   0.690084   0.661794                0.418469  0.476635  0.227181\n",
    "# GLM_grid_1_AutoML_20181212_105540_model_1            0.682648   0.63852                 0.397234  0.472683  0.223429\n",
    "#\n",
    "# [22 rows x 6 columns]\n",
    "\n",
    "# The leader model is stored here\n",
    "aml.leader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H2O AutoML Binary Classification Demo\n",
    "\n",
    "* Start H2O\n",
    "Import the h2o Python module and H2OAutoML class and initialize a local H2O cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load Data\n",
    "For the AutoML binary classification demo, we use a subset of the Product Backorders dataset. The goal here is to predict whether or not a product will be put on backorder status, given a number of product metrics such as current inventory, transit time, demand forecasts and prior sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use local data file or download from GitHub\n",
    "import os\n",
    "docker_data_path = \"/home/h2o/data/automl/product_backorders.csv\"\n",
    "if os.path.isfile(docker_data_path):\n",
    "    data_path = docker_data_path\n",
    "else:\n",
    "    data_path = \"https://github.com/h2oai/h2o-tutorials/raw/master/h2o-world-2017/automl/data/product_backorders.csv\"\n",
    "\n",
    "\n",
    "# Load data into H2O\n",
    "df = h2o.import_file(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas DataFrames should be converted to H2O dataframes before calling H2OAutoML().\n",
    "\n",
    "Note: if you don't have to preprocess the data, you can get H2O dataframes directly from the data files by a call like df = h2o.import_file(datafile_path) (where datafile_path is a filesystem path or a URL)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_y_train_h = h2o.H2OFrame(pd.concat([X_train, y_train], axis='columns'))\n",
    "X_y_train_h['dep_delayed_15min'] = X_y_train_h['dep_delayed_15min'].asfactor()\n",
    "# ^ the target column should have categorical type for classification tasks\n",
    "#   (numerical type for regression tasks)\n",
    "\n",
    "X_test_h = h2o.H2OFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will notice that the response column, \"went_on_backorder\", is already encoded as \"enum\", so there's nothing we need to do here. If it were encoded as a 0/1 \"int\", then we'd have to convert the column as follows: df[y] = df[y].asfactor()\n",
    "\n",
    "Next, let's identify the response & predictor columns by saving them as x and y. The \"sku\" column is a unique identifier so we'll want to remove that from the set of our predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \"went_on_backorder\"\n",
    "x = df.columns\n",
    "x.remove(y)\n",
    "x.remove(\"sku\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run AutoML\n",
    "\n",
    "Run AutoML, stopping after 10 models. The max_models argument specifies the number of individual (or \"base\") models, and does not include the two ensemble models that are trained at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml = H2OAutoML(\n",
    "    max_runtime_secs=(3600 * 1),  # 1 hour\n",
    "    max_models=None,  # no limit\n",
    "    seed=17\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the most important arguments (with their default values) of H2OAutoML() are the following:\n",
    "\n",
    "* nfolds=5 -- number of folds for k-fold cross-validation (nfolds=0 disables cross-validation)\n",
    "* balance_classes=False -- balance training data class counts via over/under-sampling\n",
    "* max_runtime_secs=3600 -- how long the AutoML run will execute (in seconds)\n",
    "* max_models=None -- the maximum number of models to build in an AutoML run (None means no limitation)\n",
    "* include_algos=None -- list of algorithms to restrict to during the model-building phase (cannot be used in combination with exclude_algos parameter; None means that all appropriate H2O algorithms will be used)\n",
    "* exclude_algos=None -- list of algorithms to skip during the model-building phase (None means that all appropriate H2O algorithms will be used)\n",
    "* seed=None -- a random seed for reproducibility (AutoML can only guarantee reproducibility if max_models or early stopping is used because max_runtime_secs is resource limited, meaning that if the resources are not the same between runs, AutoML may be able to train more models on one run vs another)\n",
    "\n",
    "H2O AutoML trains and cross-validates:\n",
    "\n",
    "* a default Random Forest (DRF),\n",
    "* an Extremely-Randomized Forest (XRT),\n",
    "* a random grid of Generalized Linear Models (GLM),\n",
    "* a random grid of XGBoost (XGBoost),\n",
    "* a random grid of Gradient Boosting Machines (GBM),\n",
    "* a random grid of Deep Neural Nets (DeepLearning),\n",
    "* and 2 Stacked Ensembles, one of all the models, and one of only the best models of each kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aml.train(x = x, y = y, training_frame = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaderboard\n",
    "\n",
    "Next, we will view the AutoML Leaderboard. Since we did not specify a leaderboard_frame in the H2OAutoML.train() method for scoring and ranking the models, the AutoML leaderboard uses cross-validation metrics to rank the models.\n",
    "\n",
    "A default performance metric for each machine learning task (binary classification, multiclass classification, regression) is specified internally and the leaderboard will be sorted by that metric. In the case of binary classification, the default ranking metric is Area Under the ROC Curve (AUC). In the future, the user will be able to specify any of the H2O metrics so that different metrics can be used to generate rankings on the leaderboard.\n",
    "\n",
    "The leader model is stored at aml.leader and the leaderboard is stored at aml.leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = aml.leaderboard\n",
    "model_ids = list(lb['model_id'].as_data_frame().iloc[:,0])\n",
    "out_path = \".\"\n",
    "\n",
    "for m_id in model_ids:\n",
    "    mdl = h2o.get_model(m_id)\n",
    "    h2o.save_model(model=mdl, path=out_path, force=True)\n",
    "\n",
    "h2o.export_file(lb, os.path.join(out_path, 'aml_leaderboard.h2o'), force=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To view the entire leaderboard, specify the rows argument of the head() method as the total number of rows:\n",
    "lb.head(rows=lb.nrows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Exploration\n",
    "\n",
    "To understand how the ensemble works, let's take a peek inside the Stacked Ensemble \"All Models\" model. The \"All Models\" ensemble is an ensemble of all of the individual models in the AutoML run. This is often the top performing model on the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model ids for all models in the AutoML Leaderboard\n",
    "model_ids = list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])\n",
    "# Get the \"All Models\" Stacked Ensemble model\n",
    "se = h2o.get_model([mid for mid in model_ids if \"StackedEnsemble_AllModels\" in mid][0])\n",
    "# Get the Stacked Ensemble metalearner model\n",
    "metalearner = h2o.get_model(se.metalearner()['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the variable importance of the metalearner (combiner) algorithm in the ensemble. This shows us how much each base learner is contributing to the ensemble. The AutoML Stacked Ensembles use the default metalearner algorithm (GLM with non-negative weights), so the variable importance of the metalearner is actually the standardized coefficient magnitudes of the GLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metalearner.coef_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "metalearner.std_coef_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Leader Model\n",
    "There are two ways to save the leader model -- binary format and MOJO format. If you're taking your leader model to production, then we'd suggest the MOJO format since it's optimized for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.save_model(aml.leader, path = \"./product_backorders_model_bin\")\n",
    "aml.leader.download_mojo(path = \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lb = h2o.import_file(path=os.path.join(models_path, \"aml_leaderboard.h2o\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
